{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce6ddb8",
   "metadata": {},
   "source": [
    "# Codeforces Problem and Editorial Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc530bd3",
   "metadata": {},
   "source": [
    "This notebook contains the code for scraping problems and editorials from Codeforces using Selenium and BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = \"codeforces_scraper\"\n",
    "PROBLEMS_DIR = os.path.join(BASE_DIR, \"problems\")\n",
    "EDITORIALS_DIR = os.path.join(BASE_DIR, \"editorials\")\n",
    "os.makedirs(PROBLEMS_DIR, exist_ok=True)\n",
    "os.makedirs(EDITORIALS_DIR, exist_ok=True)\n",
    "\n",
    "# Helper Functions\n",
    "def save_to_file(directory, filename, content):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "def save_metadata(directory, metadata):\n",
    "    file_path = os.path.join(directory, \"metadata.json\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "def scrape_problem(url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    \n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"problem-statement\"))\n",
    "        )\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        title = soup.find(\"div\", class_=\"title\")\n",
    "        statement = soup.find(\"div\", class_=\"problem-statement\")\n",
    "        tags = soup.find_all(\"span\", class_=\"tag-box\")\n",
    "        time_limit = soup.find(\"div\", class_=\"time-limit\")\n",
    "        memory_limit = soup.find(\"div\", class_=\"memory-limit\")\n",
    "        \n",
    "        title_text = title.text.strip() if title else \"Untitled\"\n",
    "        statement_text = statement.prettify() if statement else \"No statement found\"\n",
    "        tags_text = [tag.text.strip() for tag in tags] if tags else []\n",
    "        time_limit_text = time_limit.text.strip() if time_limit else \"Not specified\"\n",
    "        memory_limit_text = memory_limit.text.strip() if memory_limit else \"Not specified\"\n",
    "        \n",
    "        safe_title = title_text.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        save_to_file(PROBLEMS_DIR, f\"{safe_title}.txt\", statement_text)\n",
    "        metadata = {\n",
    "            \"title\": title_text,\n",
    "            \"tags\": tags_text,\n",
    "            \"time_limit\": time_limit_text,\n",
    "            \"memory_limit\": memory_limit_text,\n",
    "        }\n",
    "        save_metadata(PROBLEMS_DIR, metadata)\n",
    "        print(f\"Problem '{title_text}' scraped successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape problem at {url}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def scrape_editorial(url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    \n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"content\"))\n",
    "        )\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        title = soup.find(\"div\", class_=\"title\")\n",
    "        editorial = soup.find(\"div\", class_=\"content\")\n",
    "        \n",
    "        title_text = title.text.strip() if title else \"Untitled Editorial\"\n",
    "        editorial_text = editorial.prettify() if editorial else \"No editorial content found\"\n",
    "        \n",
    "        safe_title = title_text.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        save_to_file(EDITORIALS_DIR, f\"{safe_title}_editorial.txt\", editorial_text)\n",
    "        print(f\"Editorial for '{title_text}' scraped successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape editorial at {url}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    problem_url = \"https://codeforces.com/problemset/problem/1/A\"\n",
    "    editorial_url = \"https://codeforces.com/blog/entry/1\"\n",
    "    \n",
    "    scrape_problem(problem_url)\n",
    "    scrape_editorial(editorial_url)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}